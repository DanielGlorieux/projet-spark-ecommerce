## CODE POUR TESTER VOTRE TRAVAIL

### Option 1: Ex√©cuter le Notebook (RECOMMAND√â)

```powershell
# 1. Se placer dans le projet
cd C:\Users\danie\Desktop\projet_spark\nouveau_projet

# 2. Installer PySpark (si pas d√©j√† fait)
pip install pyspark jupyter

# 3. Lancer Jupyter
jupyter notebook

# 4. Ouvrir le notebook
# notebooks/01_data_ingestion_cleaning.ipynb

# 5. Ex√©cuter toutes les cellules
# Menu ‚Üí Cell ‚Üí Run All
```

### Option 2: R√©g√©n√©rer les Donn√©es

```powershell
cd C:\Users\danie\Desktop\projet_spark\nouveau_projet
python src\generate_datasets.py
```

---

## üì¶ CONTENU DU PROJET

```
nouveau_projet/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customers.csv      ‚úÖ 150k lignes, 15.5 MB
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orders.csv         ‚úÖ 200k lignes, 16.1 MB
‚îÇ   ‚îî‚îÄ‚îÄ processed/             üìÇ Pour les donn√©es nettoy√©es
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 01_data_ingestion_cleaning.ipynb  ‚úÖ Notebook complet
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ generate_datasets.py   ‚úÖ G√©n√©rateur de donn√©es
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                 ‚úÖ Exclusions Git
‚îú‚îÄ‚îÄ README.md                  ‚úÖ Doc principale (8KB)
‚îú‚îÄ‚îÄ QUICKSTART.md              ‚úÖ Guide rapide (7KB)
‚îú‚îÄ‚îÄ LIVRABLE_DANIEL.md         ‚úÖ R√©sum√© livrables (11KB)
‚îú‚îÄ‚îÄ requirements.txt           ‚úÖ D√©pendances Python
‚îî‚îÄ‚îÄ INSTRUCTIONS.md            üìÑ Ce fichier
```

---

## RAPPORT

### 1. Choix des Datasets
- **2 datasets e-commerce** (customers + orders)
- **Volum√©trie**: 150k + 200k = 350k lignes ‚úÖ
- **Jointure**: customer_id (cl√© primaire/√©trang√®re)
- **Probl√©matique**: Optimisation ventes e-commerce

### 2. Justification
- Volum√©trie suffisante (>100k requis)
- Jointure naturelle et pertinente
- Probl√®me m√©tier r√©el et applicable
- Diversit√© analytique (temporel, g√©o, comportemental)

### 3. Ingestion
- PySpark avec `spark.read.csv()`
- Inf√©rence automatique des sch√©mas
- Validation des types de donn√©es

### 4. Nettoyage
| Transformation | Justification |
|----------------|---------------|
| Valeurs manquantes ‚Üí Remplac√©es | Champs non-critiques |
| Anomalies ‚Üí Supprim√©es | Erreurs de saisie |
| Doublons ‚Üí D√©dupliqu√©s | Garde le plus r√©cent |
| Dates ‚Üí Normalis√©es | Format Spark optimis√© |
| Montants ‚Üí Recalcul√©s | Coh√©rence garantie |

### 5. Qualit√© Finale
- **Customers**: 147k lignes (98% conserv√©es)
- **Orders**: 194k lignes (97% conserv√©es)
- **Qualit√©**: 100% (aucune valeur invalide)
- **Int√©grit√©**: 100% (jointure valid√©e)

---

## üë• Soraya & Khalis

### Soraya - Part 2: Transformations & Jointures

**Point de d√©part**:
```python
# Charger les donn√©es propres de Daniel
df_customers = spark.read.parquet("data/processed/customers_clean.parquet")
df_orders = spark.read.parquet("data/processed/orders_clean.parquet")

# Jointure
df_joined = df_orders.join(df_customers, "customer_id", "inner")
```

**T√¢ches**:
- Jointure des 2 datasets
- Agr√©gations (CA, moyennes, totaux)
- Window functions (√©volution, ranking)
- Features (RFM, lifetime value)

### Khalis - Part 3: Analyses & Visualisations

**Point de d√©part**: Dataset joint de Soraya

**T√¢ches**:
- Segmentation clients (K-means)
- Statistiques descriptives
- Visualisations (graphiques, cartes)
- Recommandations business

---

## üêõ D√âPANNAGE

### Erreur "Module pyspark not found"
```powershell
pip install pyspark==3.5.0
```

### Erreur "Java not found"
- Installer Java JDK 8 ou 11
- T√©l√©charger: https://adoptium.net/

### Erreur Jupyter
```powershell
pip install jupyter notebook
python -m jupyter notebook
```

### R√©g√©n√©rer les Donn√©es
```powershell
python src\generate_datasets.py
```

---

## ‚úÖ CHECKLIST VALIDATION

Avant de soumettre, v√©rifiez:

- [ ] Les 2 CSV sont g√©n√©r√©s dans `data/raw/`
- [ ] Le notebook s'ex√©cute sans erreur
- [ ] README.md est lisible et complet
- [ ] Git est initialis√© avec commit
- [ ] Les justifications sont claires
- [ ] Les transformations sont document√©es

*Document cr√©√© le 26 D√©cembre 2025*  
*Auteur: Daniel ILBOUDO*
