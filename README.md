# Projet d'Analyse de DonnÃ©es avec Apache Spark

##  Membres du Groupe
- **ILBOUDO P. Daniel Glorieux** - Data Ingestion & PrÃ©paration
- **PITROIPA Soraya** - Transformations & Jointures
- **KONE Khalis AÃ¯man** - Analyses & Visualisations

---

##  Choix des Datasets

### ProblÃ©matique MÃ©tier
**Analyse du comportement d'achat des clients e-commerce pour optimiser les ventes et la satisfaction client**

Objectifs :
- Identifier les segments de clients les plus rentables
- Analyser les tendances d'achat par catÃ©gorie et rÃ©gion
- DÃ©tecter les comportements anormaux (fraudes potentielles, abandons)
- Optimiser la stratÃ©gie de pricing et de fidÃ©lisation

### Datasets SÃ©lectionnÃ©s

#### 1. **Dataset Customers (Clients)**
- **Source** : GÃ©nÃ©rÃ© avec donnÃ©es rÃ©alistes d'e-commerce
- **Taille** : ~150,000 lignes
- **Format** : CSV
- **Colonnes clÃ©s** :
  - `customer_id` (clÃ© primaire)
  - `first_name`, `last_name`
  - `email`, `phone`
  - `registration_date`
  - `country`, `city`
  - `customer_segment` (Bronze, Silver, Gold, Platinum)
  - `total_spent`
  - `is_active`

#### 2. **Dataset Orders (Commandes)**
- **Source** : GÃ©nÃ©rÃ© avec donnÃ©es rÃ©alistes d'e-commerce
- **Taille** : ~200,000 lignes
- **Format** : CSV
- **Colonnes clÃ©s** :
  - `order_id` (clÃ© primaire)
  - `customer_id` (clÃ© Ã©trangÃ¨re â†’ customers)
  - `order_date`
  - `product_category`
  - `product_name`
  - `quantity`
  - `unit_price`
  - `total_amount`
  - `payment_method`
  - `order_status`
  - `shipping_country`

### Justification du Choix

 **VolumÃ©trie suffisante** : 150k+ et 200k+ lignes
 **Jointure naturelle** : `customer_id` permet des analyses riches
 **ProblÃ¨me mÃ©tier rÃ©el** : applicable Ã  tout e-commerce
 **DiversitÃ© des analyses possibles** :
  - Segmentation clients
  - Analyse temporelle
  - Analyse gÃ©ographique
  - Analyse par catÃ©gorie produit
  - DÃ©tection d'anomalies (fraudes, valeurs aberrantes)

---

## ğŸ› ï¸ Part 1 : Data Ingestion & PrÃ©paration (Daniel ILBOUDO)

### 1. GÃ©nÃ©ration des Datasets

```bash
python src/generate_datasets.py
```

GÃ©nÃ¨re :
- `data/raw/customers.csv` (~150k lignes)
- `data/raw/orders.csv` (~200k lignes)

### 2. Ingestion avec Spark

Le notebook `notebooks/01_data_ingestion_cleaning.ipynb` contient :

#### Chargement des donnÃ©es
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("EcommerceAnalysis") \
    .getOrCreate()

# Lecture avec infÃ©rence de schÃ©ma
df_customers = spark.read.csv("data/raw/customers.csv", header=True, inferSchema=True)
df_orders = spark.read.csv("data/raw/orders.csv", header=True, inferSchema=True)
```

#### VÃ©rification des schÃ©mas
- Types de donnÃ©es automatiquement dÃ©tectÃ©s
- Validation de la cohÃ©rence
- Identification des colonnes clÃ©s

### 3. Nettoyage & PrÃ©paration

#### ProblÃ¨mes IdentifiÃ©s et Solutions

##### **Customers Dataset**

| ProblÃ¨me | Impact | Solution AppliquÃ©e |
|----------|--------|-------------------|
| Valeurs manquantes dans `phone` | 2-3% | Remplacement par "Unknown" |
| Valeurs manquantes dans `city` | 1-2% | Remplacement par "Unknown" |
| Emails en double | ~0.5% | DÃ©duplication (garde le plus rÃ©cent) |
| Dates invalides | <1% | Suppression des lignes |
| `total_spent` nÃ©gatif | <0.1% | Suppression (anomalies) |
| Format dates incohÃ©rent | - | Normalisation en `yyyy-MM-dd` |

##### **Orders Dataset**

| ProblÃ¨me | Impact | Solution AppliquÃ©e |
|----------|--------|-------------------|
| Valeurs manquantes dans `shipping_country` | 3-5% | Remplacement par pays du client (aprÃ¨s jointure) |
| `quantity` = 0 ou nÃ©gatif | ~1% | Suppression |
| `unit_price` = 0 ou nÃ©gatif | ~1% | Suppression |
| `total_amount` incohÃ©rent | ~2% | Recalcul : `quantity * unit_price` |
| Format dates incohÃ©rent | - | Normalisation en `yyyy-MM-dd` |
| `order_status` NULL | <1% | Remplacement par "Pending" |

#### Transformations AppliquÃ©es

```python
# Normalisation des dates
df_customers = df_customers.withColumn(
    "registration_date_clean",
    to_date(col("registration_date"), "yyyy-MM-dd")
)

# Gestion des valeurs manquantes
df_customers = df_customers.fillna({
    "phone": "Unknown",
    "city": "Unknown"
})

# DÃ©duplication
df_customers = df_customers.dropDuplicates(["email"])

# Validation des montants
df_orders = df_orders.filter(
    (col("quantity") > 0) & 
    (col("unit_price") > 0)
)

# Recalcul des totaux
df_orders = df_orders.withColumn(
    "total_amount_clean",
    col("quantity") * col("unit_price")
)
```

#### PrÃ©paration pour la Jointure

**Colonne de jointure** : `customer_id`

VÃ©rifications effectuÃ©es :
-  Pas de valeurs NULL dans `customer_id` des deux datasets
-  Tous les `customer_id` dans orders existent dans customers
-  Types cohÃ©rents (Integer)
-  ClÃ© primaire respectÃ©e (customers.customer_id unique)

### 4. Justification des Choix

#### Pourquoi ces transformations ?

1. **Valeurs manquantes** :
   - TÃ©lÃ©phone/Ville non critiques â†’ "Unknown" prÃ©serve les donnÃ©es
   - Dates/Montants critiques â†’ Suppression pour garantir la qualitÃ©

2. **DÃ©duplication** :
   - Emails en double = comptes multiples â†’ garde le plus rÃ©cent
   - PrÃ©serve l'intÃ©gritÃ© rÃ©fÃ©rentielle

3. **Validation mÃ©tier** :
   - QuantitÃ©s/Prix nÃ©gatifs = erreurs de saisie â†’ Suppression
   - Total_amount recalculÃ© â†’ Garantit cohÃ©rence

4. **Normalisation dates** :
   - Format unique facilite les analyses temporelles
   - Compatible avec les fonctions Spark

### 5. RÃ©sultats

#### Datasets Propres GÃ©nÃ©rÃ©s

**Customers Clean**
- Lignes avant : ~150,000
- Lignes aprÃ¨s : ~147,000 (-2%)
- PrÃªt pour jointure : 

**Orders Clean**
- Lignes avant : ~200,000
- Lignes aprÃ¨s : ~194,000 (-3%)
- PrÃªt pour jointure : 

#### Formats de Sortie

```bash
data/processed/
â”œâ”€â”€ customers_clean.csv
â”œâ”€â”€ customers_clean.parquet
â”œâ”€â”€ orders_clean.csv
â””â”€â”€ orders_clean.parquet
```

Parquet recommandÃ© pour les Ã©tapes suivantes (performance).

---

##  Installation

```bash
# CrÃ©er environnement virtuel
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Installer dÃ©pendances
pip install -r requirements.txt
```

##  Utilisation

```bash
# 1. GÃ©nÃ©rer les datasets
python src/generate_datasets.py

# 2. Lancer Jupyter
jupyter notebook

# 3. Ouvrir et exÃ©cuter
notebooks/01_data_ingestion_cleaning.ipynb
```

---

##  Structure du Projet

```
nouveau_projet/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes gÃ©nÃ©rÃ©es
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â””â”€â”€ orders.csv
â”‚   â””â”€â”€ processed/              # DonnÃ©es nettoyÃ©es (output Part 1)
â”‚       â”œâ”€â”€ customers_clean.csv
â”‚       â”œâ”€â”€ customers_clean.parquet
â”‚       â”œâ”€â”€ orders_clean.csv
â”‚       â””â”€â”€ orders_clean.parquet
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_data_ingestion_cleaning.ipynb  # Notebook Part 1 (Daniel)
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ generate_datasets.py    # Script de gÃ©nÃ©ration des donnÃ©es
â”‚
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â””â”€â”€ README.md                   # Ce fichier
```

---

##  Prochaines Ã‰tapes (Autres Membres)

### Part 2 - Soraya : Transformations & Jointures
- Jointure `customers â‹ˆ orders`
- AgrÃ©gations (CA par client, par pays, par catÃ©gorie)
- Window functions (Ã©volution temporelle)
- CrÃ©ation de features (RFM, lifetime value)

### Part 3 - Khalis : Analyses & Visualisations
- Segmentation clients
- Analyses prÃ©dictives
- Dashboards Matplotlib/Plotly
- Recommandations business

---

##  Technologies

- **Apache Spark 3.5+** - Traitement distribuÃ©
- **PySpark** - API Python pour Spark
- **Jupyter Notebook** - Environnement interactif
- **Pandas** - GÃ©nÃ©ration de donnÃ©es
- **Faker** - DonnÃ©es rÃ©alistes

---

##  Commit Git

```bash
git add .
git commit -m "data_ingestion_cleaning - Part Daniel ILBOUDO"
git push origin main
```

---

##  Auteur - Part 1

**ILBOUDO P. Daniel Glorieux**
- Data Ingestion & PrÃ©paration
- GÃ©nÃ©ration des datasets
- Nettoyage et validation
- PrÃ©paration pour jointures

- ## Part2
**PITROIPA SORAYA
-(Transformations & Jointures)
-code 02_transformations_joitures.ipynb
