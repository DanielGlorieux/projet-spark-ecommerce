{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ce5d6b",
   "metadata": {},
   "source": [
    "# PART 2 - Transformations & Jointures avec PySpark\n",
    "## Projet E-Commerce - Soraya PITROIPA\n",
    "\n",
    "---\n",
    "\n",
    "**Objectifs** :\n",
    "- Effectuer des jointures entre les datasets Customers et Orders\n",
    "- Réaliser des agrégations complexes (CA par client, catégorie, pays)\n",
    "- Utiliser les fonctions de fenêtrage (Window Functions)\n",
    "- Calculer des métriques métier (RFM, clients actifs, taux de rétention)\n",
    "- Créer des visualisations des résultats\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ba719",
   "metadata": {},
   "source": [
    "## 1. Configuration Spark & Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155ec4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Variables d'environnement configurées\n",
      "   JAVA_HOME: C:\\Program Files\\Microsoft\\jdk-17.0.17.10-hotspot\n",
      "   HADOOP_HOME: C:\\hadoop\n"
     ]
    }
   ],
   "source": [
    "# Configuration des variables d'environnement pour Spark sur Windows\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Microsoft\\jdk-17.0.17.10-hotspot\"\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "\n",
    "print(\" Variables d'environnement configurées\")\n",
    "print(f\"   JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"   HADOOP_HOME: {os.environ['HADOOP_HOME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a941b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    datediff, current_date, when, round as spark_round,\n",
    "    row_number, rank, dense_rank, lag, lead,\n",
    "    year, month, dayofmonth, to_date\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Part2_Transformations_Jointures\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e606b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fichiers trouvés dans: ./data/processed/customers_clean.parquet\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:420)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(customers_path) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(orders_path):\n\u001b[0;32m     16\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Fichiers trouvés dans: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustomers_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \tdf_customers \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(customers_path)\n\u001b[0;32m     18\u001b[0m \tdf_orders \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(orders_path)\n\u001b[0;32m     19\u001b[0m \t\u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:642\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    631\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    633\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    634\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    640\u001b[0m )\n\u001b[1;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mparquet(_to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc, paths)))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:420)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données nettoyées (Part 1)\n",
    "import os\n",
    "\n",
    "# Vérifier les chemins possibles\n",
    "paths_to_try = [\n",
    "\t(\"../data/processed/customers_clean.parquet\", \"../data/processed/orders_clean.parquet\"),\n",
    "\t(\"./data/processed/customers_clean.parquet\", \"./data/processed/orders_clean.parquet\"),\n",
    "\t(\"data/processed/customers_clean.parquet\", \"data/processed/orders_clean.parquet\"),\n",
    "]\n",
    "\n",
    "df_customers = None\n",
    "df_orders = None\n",
    "\n",
    "for customers_path, orders_path in paths_to_try:\n",
    "\tif os.path.exists(customers_path) and os.path.exists(orders_path):\n",
    "\t\tprint(f\"✓ Fichiers trouvés dans: {customers_path}\")\n",
    "\t\tdf_customers = spark.read.parquet(customers_path)\n",
    "\t\tdf_orders = spark.read.parquet(orders_path)\n",
    "\t\tbreak\n",
    "\n",
    "if df_customers is None or df_orders is None:\n",
    "\tprint(\"❌ Fichiers parquet non trouvés. Veuillez vérifier que Part 1 a été exécutée.\")\n",
    "\tprint(f\"Répertoire actuel: {os.getcwd()}\")\n",
    "\tprint(\"\\nVeuillez exécuter le notebook Part 1 d'abord pour générer les fichiers nettoyés.\")\n",
    "else:\n",
    "\tprint(f\"Customers: {df_customers.count():,} lignes\")\n",
    "\tprint(f\"Orders: {df_orders.count():,} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema Customers ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Vérification des schémas\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Schema Customers ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_customers\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Schema Orders ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m df_orders\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "# Vérification des schémas\n",
    "print(\"=== Schema Customers ===\")\n",
    "df_customers.printSchema()\n",
    "\n",
    "print(\"\\n=== Schema Orders ===\")\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324986dc",
   "metadata": {},
   "source": [
    "## 2. Jointure Customers ⋈ Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7447f7",
   "metadata": {},
   "source": [
    "###  Justification de la Jointure\n",
    "\n",
    "**Type de jointure : INNER JOIN**\n",
    "\n",
    "**Raison du choix :**\n",
    "- Nous voulons analyser uniquement les commandes associées à des clients valides\n",
    "- Élimine les commandes orphelines (customer_id inexistant dans customers)\n",
    "- Garantit l'intégrité référentielle des analyses\n",
    "\n",
    "**Cardinalité : 1-N (One-to-Many)**\n",
    "- **1 Client** peut avoir **N Commandes** (0, 1 ou plusieurs)\n",
    "- Relation classique dans un système e-commerce\n",
    "- La clé de jointure `customer_id` est :\n",
    "  - **Clé primaire** dans `customers` (unicité garantie)\n",
    "  - **Clé étrangère** dans `orders` (répétitions autorisées)\n",
    "\n",
    "**Impact attendu :**\n",
    "- Perte de commandes sans client valide (données incohérentes)\n",
    "- Conservation de tous les clients actifs ayant passé au moins 1 commande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959f21e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Jointure INNER (garder uniquement les commandes avec client valide)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_joined \u001b[38;5;241m=\u001b[39m df_orders\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m      3\u001b[0m     df_customers,\n\u001b[0;32m      4\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLignes après jointure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_joined\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColonnes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_joined\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "# Jointure INNER (garder uniquement les commandes avec client valide)\n",
    "df_joined = df_orders.join(\n",
    "    df_customers,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Lignes après jointure: {df_joined.count():,}\")\n",
    "print(f\"Colonnes: {len(df_joined.columns)}\")\n",
    "\n",
    "# Aperçu\n",
    "df_joined.select(\n",
    "    \"order_id\", \"customer_id\", \"first_name\", \"last_name\",\n",
    "    \"order_date\", \"product_category\", \"total_amount\", \"customer_segment\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vérification Jointure ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Vérification de la qualité de la jointure\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Vérification Jointure ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrders avant jointure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_orders\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrders après jointure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_joined\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerte: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((df_orders\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mdf_joined\u001b[38;5;241m.\u001b[39mcount())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mdf_orders\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "# Vérification de la qualité de la jointure\n",
    "print(\"=== Vérification Jointure ===\")\n",
    "print(f\"Orders avant jointure: {df_orders.count():,}\")\n",
    "print(f\"Orders après jointure: {df_joined.count():,}\")\n",
    "print(f\"Perte: {((df_orders.count() - df_joined.count()) / df_orders.count() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f373690",
   "metadata": {},
   "source": [
    "## 3. Agrégations Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09276900",
   "metadata": {},
   "source": [
    "### 3.1 Chiffre d'Affaires par Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA par client avec statistiques\n",
    "df_ca_client = df_joined.groupBy(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"ca_moyen\"),\n",
    "    spark_max(\"total_amount\").alias(\"ca_max\"),\n",
    "    spark_min(\"total_amount\").alias(\"ca_min\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "# Arrondir les valeurs\n",
    "df_ca_client = df_ca_client.withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    "    .withColumn(\"ca_moyen\", spark_round(\"ca_moyen\", 2)) \\\n",
    "    .withColumn(\"ca_max\", spark_round(\"ca_max\", 2)) \\\n",
    "    .withColumn(\"ca_min\", spark_round(\"ca_min\", 2))\n",
    "\n",
    "print(\"=== TOP 10 Clients par CA ===\")\n",
    "df_ca_client.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques globales\n",
    "print(\"=== Statistiques CA Global ===\")\n",
    "df_ca_client.select(\n",
    "    spark_sum(\"ca_total\").alias(\"ca_total_global\"),\n",
    "    avg(\"ca_total\").alias(\"ca_moyen_par_client\"),\n",
    "    avg(\"nb_commandes\").alias(\"nb_commandes_moyen\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b1af4",
   "metadata": {},
   "source": [
    "### 3.2 Chiffre d'Affaires par Pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA par pays\n",
    "df_ca_pays = df_joined.groupBy(\"country\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"ca_moyen\"),\n",
    "    count(\"customer_id\").alias(\"nb_clients_actifs\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_ca_pays = df_ca_pays.withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    "    .withColumn(\"ca_moyen\", spark_round(\"ca_moyen\", 2))\n",
    "\n",
    "print(\"=== TOP 10 Pays par CA ===\")\n",
    "df_ca_pays.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8486465",
   "metadata": {},
   "source": [
    "### 3.3 Chiffre d'Affaires par Catégorie de Produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA par catégorie\n",
    "df_ca_categorie = df_joined.groupBy(\"product_category\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"ca_moyen\"),\n",
    "    spark_sum(\"quantity\").alias(\"quantite_totale\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_ca_categorie = df_ca_categorie.withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    "    .withColumn(\"ca_moyen\", spark_round(\"ca_moyen\", 2))\n",
    "\n",
    "print(\"=== CA par Catégorie de Produit ===\")\n",
    "df_ca_categorie.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7806d2",
   "metadata": {},
   "source": [
    "### 3.4 Chiffre d'Affaires par Segment Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA par segment client\n",
    "df_ca_segment = df_joined.groupBy(\"customer_segment\").agg(\n",
    "    count(\"customer_id\").alias(\"nb_clients\"),\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"ca_moyen_commande\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_ca_segment = df_ca_segment.withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    "    .withColumn(\"ca_moyen_commande\", spark_round(\"ca_moyen_commande\", 2))\n",
    "\n",
    "print(\"=== CA par Segment Client ===\")\n",
    "df_ca_segment.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce373448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panier moyen par client\n",
    "df_panier_moyen = df_joined.groupBy(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"panier_moyen\")\n",
    ").withColumn(\"panier_moyen\", spark_round(\"panier_moyen\", 2)) \\\n",
    " .withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    " .orderBy(col(\"panier_moyen\").desc())\n",
    "\n",
    "print(\"=== TOP 10 Clients par Panier Moyen ===\")\n",
    "df_panier_moyen.show(10, truncate=False)\n",
    "\n",
    "# Panier moyen global\n",
    "panier_moyen_global = df_joined.agg(avg(\"total_amount\").alias(\"panier_moyen_global\")).collect()[0][0]\n",
    "print(f\"\\n Panier Moyen Global : {panier_moyen_global:.2f} €\")\n",
    "\n",
    "# Panier moyen par segment\n",
    "df_panier_segment = df_joined.groupBy(\"customer_segment\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    avg(\"total_amount\").alias(\"panier_moyen\")\n",
    ").withColumn(\"panier_moyen\", spark_round(\"panier_moyen\", 2)) \\\n",
    " .orderBy(col(\"panier_moyen\").desc())\n",
    "\n",
    "print(\"\\n=== Panier Moyen par Segment ===\")\n",
    "df_panier_segment.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd613a47",
   "metadata": {},
   "source": [
    "### 3.5 Panier Moyen\n",
    "\n",
    "Le **panier moyen** représente le montant moyen dépensé par commande. C'est un indicateur clé de performance (KPI) pour mesurer la valeur moyenne des transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deadc98",
   "metadata": {},
   "source": [
    "## 4. Window Functions - Analyses Temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul RFM par client\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "# 1. Calculer les métriques RFM pour chaque client\n",
    "df_rfm_base = df_joined.groupBy(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\").agg(\n",
    "    datediff(current_date(), spark_max(\"order_date\")).alias(\"recency\"),  # Jours depuis dernier achat\n",
    "    count(\"order_id\").alias(\"frequency\"),  # Nombre de commandes\n",
    "    spark_sum(\"total_amount\").alias(\"monetary\")  # CA total\n",
    ").withColumn(\"monetary\", spark_round(\"monetary\", 2))\n",
    "\n",
    "print(\"=== Métriques RFM Brutes ===\")\n",
    "df_rfm_base.orderBy(col(\"monetary\").desc()).show(10, truncate=False)\n",
    "\n",
    "# 2. Calculer les scores RFM (1-5) avec ntile\n",
    "# Note: Pour Recency, plus le score est élevé, plus c'est récent (inverse de la valeur)\n",
    "window_spec = Window.orderBy(col(\"recency\"))\n",
    "window_spec_desc = Window.orderBy(col(\"frequency\").desc())\n",
    "window_spec_desc2 = Window.orderBy(col(\"monetary\").desc())\n",
    "\n",
    "df_rfm_scores = df_rfm_base.withColumn(\n",
    "    \"R_score\", \n",
    "    6 - ntile(5).over(window_spec)  # Inverser: récent = score élevé\n",
    ").withColumn(\n",
    "    \"F_score\",\n",
    "    ntile(5).over(window_spec_desc)\n",
    ").withColumn(\n",
    "    \"M_score\",\n",
    "    ntile(5).over(window_spec_desc2)\n",
    ")\n",
    "\n",
    "# 3. Créer le score RFM combiné\n",
    "df_rfm_final = df_rfm_scores.withColumn(\n",
    "    \"RFM_score\",\n",
    "    col(\"R_score\") * 100 + col(\"F_score\") * 10 + col(\"M_score\")\n",
    ").withColumn(\n",
    "    \"RFM_segment\",\n",
    "    when(col(\"RFM_score\") >= 444, \"Champions\")  # 444-555\n",
    "    .when(col(\"RFM_score\") >= 344, \"Loyal Customers\")  # 344-443\n",
    "    .when(col(\"RFM_score\") >= 244, \"Potential Loyalists\")  # 244-343\n",
    "    .when(col(\"RFM_score\") >= 144, \"At Risk\")  # 144-243\n",
    "    .otherwise(\"Lost\")  # < 144\n",
    ").orderBy(col(\"RFM_score\").desc())\n",
    "\n",
    "print(\"\\n=== TOP 20 Clients - Analyse RFM ===\")\n",
    "df_rfm_final.select(\n",
    "    \"customer_id\", \"first_name\", \"last_name\", \n",
    "    \"recency\", \"frequency\", \"monetary\",\n",
    "    \"R_score\", \"F_score\", \"M_score\", \"RFM_score\", \"RFM_segment\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "# 4. Distribution par segment RFM\n",
    "df_rfm_distribution = df_rfm_final.groupBy(\"RFM_segment\").agg(\n",
    "    count(\"customer_id\").alias(\"nb_clients\"),\n",
    "    avg(\"monetary\").alias(\"ca_moyen\"),\n",
    "    spark_sum(\"monetary\").alias(\"ca_total\")\n",
    ").withColumn(\"ca_moyen\", spark_round(\"ca_moyen\", 2)) \\\n",
    " .withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    " .orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "print(\"\\n=== Distribution par Segment RFM ===\")\n",
    "df_rfm_distribution.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54960d",
   "metadata": {},
   "source": [
    "## 3.6 Analyse RFM (Recency, Frequency, Monetary)\n",
    "\n",
    "**RFM** est une méthode de segmentation client basée sur 3 dimensions :\n",
    "- **R - Recency (Récence)** : Temps écoulé depuis le dernier achat (plus récent = meilleur)\n",
    "- **F - Frequency (Fréquence)** : Nombre de commandes (plus élevé = meilleur)\n",
    "- **M - Monetary (Montant)** : CA total généré (plus élevé = meilleur)\n",
    "\n",
    "Chaque dimension est notée de 1 à 5 (5 = meilleur), permettant d'identifier les meilleurs clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Créer un dataset sécurisé pour export/partage\n",
    "df_customers_export = df_customers_masked_v2.select(\n",
    "    col(\"customer_id_hash\").alias(\"customer_id_anonyme\"),\n",
    "    col(\"email_hash\").alias(\"email_anonyme\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"phone_masked_v2\").alias(\"phone\"),\n",
    "    col(\"city\"),\n",
    "    col(\"state\"),\n",
    "    col(\"country\"),\n",
    "    col(\"customer_segment\"),\n",
    "    col(\"total_spent\"),\n",
    "    col(\"registration_date\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Dataset Clients Sécurisé (Prêt pour Export) ===\")\n",
    "df_customers_export.show(10, truncate=False)\n",
    "\n",
    "# Statistiques de sécurisation\n",
    "total_clients = df_customers.count()\n",
    "print(f\"\\n Sécurisation Complétée :\")\n",
    "print(f\"   - {total_clients:,} clients sécurisés\")\n",
    "print(f\"   - customer_id → Hash SHA-256 (64 caractères)\")\n",
    "print(f\"   - email → Hash SHA-256 (64 caractères)\")\n",
    "print(f\"   - phone → Masqué (XXX-XXX-{'{derniers 4 chiffres}'})\")\n",
    "print(f\"   - Conformité RGPD : Anonymisation des données personnelles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Masquage du numéro de téléphone\n",
    "# Format original : +1-234-5674-8900 ou similaire\n",
    "# Format masqué : XXX-XXX-X900 (garder les 4 derniers chiffres)\n",
    "\n",
    "def mask_phone_udf(phone):\n",
    "    \"\"\"Masque un numéro de téléphone en gardant les 4 derniers chiffres\"\"\"\n",
    "    if phone is None or phone == \"Unknown\":\n",
    "        return \"XXX-XXX-XXXX\"\n",
    "    \n",
    "    # Extraire uniquement les chiffres\n",
    "    digits_only = ''.join(filter(str.isdigit, str(phone)))\n",
    "    \n",
    "    if len(digits_only) >= 4:\n",
    "        # Garder les 4 derniers chiffres\n",
    "        last_4 = digits_only[-4:]\n",
    "        masked = f\"XXX-XXX-{last_4}\"\n",
    "    else:\n",
    "        masked = \"XXX-XXX-XXXX\"\n",
    "    \n",
    "    return masked\n",
    "\n",
    "# Enregistrer l'UDF\n",
    "from pyspark.sql.types import StringType\n",
    "mask_phone = spark.udf.register(\"mask_phone\", mask_phone_udf, StringType())\n",
    "\n",
    "# Appliquer le masquage\n",
    "df_customers_masked = df_customers_secure.withColumn(\n",
    "    \"phone_masked\",\n",
    "    mask_phone(col(\"phone\"))\n",
    ")\n",
    "\n",
    "print(\"\\n=== Masquage des Numéros de Téléphone ===\")\n",
    "df_customers_masked.select(\n",
    "    \"customer_id\", \"first_name\", \"last_name\",\n",
    "    \"phone\", \"phone_masked\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Alternative avec regex (plus performant pour Spark)\n",
    "df_customers_masked_v2 = df_customers_secure.withColumn(\n",
    "    \"phone_masked_v2\",\n",
    "    when(\n",
    "        (col(\"phone\").isNull()) | (col(\"phone\") == \"Unknown\"),\n",
    "        lit(\"XXX-XXX-XXXX\")\n",
    "    ).otherwise(\n",
    "        concat_ws(\n",
    "            \"-\",\n",
    "            lit(\"XXX\"),\n",
    "            lit(\"XXX\"),\n",
    "            regexp_replace(col(\"phone\"), r\"^.*(\\d{4})$\", \"$1\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n=== Masquage avec Regex (Version Optimisée) ===\")\n",
    "df_customers_masked_v2.select(\n",
    "    \"customer_id\", \"phone\", \"phone_masked_v2\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c507011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des fonctions de sécurité\n",
    "from pyspark.sql.functions import sha2, concat_ws, substring, length, regexp_replace, lit\n",
    "\n",
    "# 1. Hash SHA-256 pour customer_id et email\n",
    "df_customers_secure = df_customers.withColumn(\n",
    "    \"customer_id_hash\",\n",
    "    sha2(col(\"customer_id\").cast(\"string\"), 256)\n",
    ").withColumn(\n",
    "    \"email_hash\",\n",
    "    sha2(col(\"email\"), 256)\n",
    ")\n",
    "\n",
    "print(\"=== Hash SHA-256 des Identifiants ===\")\n",
    "df_customers_secure.select(\n",
    "    \"customer_id\", \"customer_id_hash\",\n",
    "    \"email\", \"email_hash\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n📊 Statistiques Hash :\")\n",
    "print(f\"   - Longueur hash SHA-256 : 64 caractères\")\n",
    "print(f\"   - Type : Chiffrement unidirectionnel (irreversible)\")\n",
    "print(f\"   - Usage : Identification anonymisée pour analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e0e2c",
   "metadata": {},
   "source": [
    "## 3.7  Sécurité et Protection des Données (RGPD)\n",
    "\n",
    "Pour protéger les données sensibles des clients, nous appliquons deux techniques :\n",
    "1. **Hash SHA-256** : Chiffrement unidirectionnel des identifiants sensibles (customer_id, email)\n",
    "2. **Masquage** : Anonymisation partielle des données personnelles (numéro de téléphone)\n",
    "\n",
    "Ces mesures assurent la conformité RGPD tout en permettant les analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d0c1a",
   "metadata": {},
   "source": [
    "### 4.1 Évolution Mensuelle du CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les colonnes de date\n",
    "df_temporal = df_joined.withColumn(\"year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"month\", month(\"order_date\"))\n",
    "\n",
    "# CA mensuel\n",
    "df_ca_mensuel = df_temporal.groupBy(\"year\", \"month\").agg(\n",
    "    spark_sum(\"total_amount\").alias(\"ca_mensuel\"),\n",
    "    count(\"order_id\").alias(\"nb_commandes\")\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "df_ca_mensuel = df_ca_mensuel.withColumn(\"ca_mensuel\", spark_round(\"ca_mensuel\", 2))\n",
    "\n",
    "print(\"=== CA Mensuel ===\")\n",
    "df_ca_mensuel.show(24, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window function: CA cumulé et évolution\n",
    "window_temporal = Window.orderBy(\"year\", \"month\")\n",
    "\n",
    "df_ca_mensuel_evolution = df_ca_mensuel.withColumn(\n",
    "    \"ca_cumule\",\n",
    "    spark_sum(\"ca_mensuel\").over(window_temporal)\n",
    ").withColumn(\n",
    "    \"ca_mois_precedent\",\n",
    "    lag(\"ca_mensuel\", 1).over(window_temporal)\n",
    ").withColumn(\n",
    "    \"evolution_pct\",\n",
    "    when(\n",
    "        col(\"ca_mois_precedent\").isNotNull(),\n",
    "        spark_round(((col(\"ca_mensuel\") - col(\"ca_mois_precedent\")) / col(\"ca_mois_precedent\") * 100), 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"=== Évolution Mensuelle du CA ===\")\n",
    "df_ca_mensuel_evolution.show(24, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444be53f",
   "metadata": {},
   "source": [
    "### 4.2 Ranking des Clients par Période"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA par client et par mois\n",
    "df_client_mensuel = df_temporal.groupBy(\"year\", \"month\", \"customer_id\", \"first_name\", \"last_name\").agg(\n",
    "    spark_sum(\"total_amount\").alias(\"ca_mensuel\")\n",
    ")\n",
    "\n",
    "# Window pour ranking par mois\n",
    "window_rank = Window.partitionBy(\"year\", \"month\").orderBy(col(\"ca_mensuel\").desc())\n",
    "\n",
    "df_top_clients_mensuel = df_client_mensuel.withColumn(\n",
    "    \"rank\",\n",
    "    rank().over(window_rank)\n",
    ").filter(col(\"rank\") <= 5)\n",
    "\n",
    "df_top_clients_mensuel = df_top_clients_mensuel.withColumn(\"ca_mensuel\", spark_round(\"ca_mensuel\", 2))\n",
    "\n",
    "print(\"=== TOP 5 Clients par Mois ===\")\n",
    "df_top_clients_mensuel.orderBy(\"year\", \"month\", \"rank\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99f91f",
   "metadata": {},
   "source": [
    "### 4.3 Nombre de Commandes par Client avec Numérotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numéroter les commandes par client (ordre chronologique)\n",
    "window_row_number = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "df_orders_numbered = df_joined.withColumn(\n",
    "    \"order_number\",\n",
    "    row_number().over(window_row_number)\n",
    ")\n",
    "\n",
    "# Première et dernière commande par client\n",
    "df_first_last = df_orders_numbered.filter(\n",
    "    (col(\"order_number\") == 1) | \n",
    "    (col(\"order_number\") == df_orders_numbered.groupBy(\"customer_id\").agg(spark_max(\"order_number\")).first()[0])\n",
    ")\n",
    "\n",
    "print(\"=== Premières et Dernières Commandes (échantillon) ===\")\n",
    "df_orders_numbered.select(\n",
    "    \"customer_id\", \"first_name\", \"last_name\", \"order_date\", \"order_number\", \"total_amount\"\n",
    ").filter(col(\"customer_id\") < 10).orderBy(\"customer_id\", \"order_number\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a665f7",
   "metadata": {},
   "source": [
    "## 5. Création de Features - RFM (Recency, Frequency, Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date de référence (dernière date dans le dataset)\n",
    "max_date = df_joined.agg(spark_max(\"order_date\")).collect()[0][0]\n",
    "print(f\"Date de référence: {max_date}\")\n",
    "\n",
    "# Calcul RFM\n",
    "df_rfm = df_joined.groupBy(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\").agg(\n",
    "    # Recency: jours depuis la dernière commande\n",
    "    datediff(to_date(spark.sql(f\"SELECT '{max_date}'\").first()[0]), spark_max(\"order_date\")).alias(\"recency\"),\n",
    "    \n",
    "    # Frequency: nombre de commandes\n",
    "    count(\"order_id\").alias(\"frequency\"),\n",
    "    \n",
    "    # Monetary: montant total dépensé\n",
    "    spark_sum(\"total_amount\").alias(\"monetary\")\n",
    ")\n",
    "\n",
    "df_rfm = df_rfm.withColumn(\"monetary\", spark_round(\"monetary\", 2))\n",
    "\n",
    "print(\"=== Analyse RFM ===\")\n",
    "df_rfm.orderBy(col(\"monetary\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3227af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation RFM par quartiles\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "window_rfm = Window.orderBy(col(\"recency\").asc())  # Plus petit = meilleur\n",
    "window_fm = Window.orderBy(col(\"frequency\").desc())  # Plus grand = meilleur\n",
    "window_m = Window.orderBy(col(\"monetary\").desc())  # Plus grand = meilleur\n",
    "\n",
    "df_rfm_score = df_rfm.withColumn(\"r_score\", ntile(4).over(window_rfm)) \\\n",
    "    .withColumn(\"f_score\", ntile(4).over(window_fm)) \\\n",
    "    .withColumn(\"m_score\", ntile(4).over(window_m))\n",
    "\n",
    "# Score global RFM\n",
    "df_rfm_score = df_rfm_score.withColumn(\n",
    "    \"rfm_score\",\n",
    "    col(\"r_score\") + col(\"f_score\") + col(\"m_score\")\n",
    ")\n",
    "\n",
    "# Catégorisation\n",
    "df_rfm_score = df_rfm_score.withColumn(\n",
    "    \"rfm_category\",\n",
    "    when(col(\"rfm_score\") >= 10, \"Champion\") \\\n",
    "    .when(col(\"rfm_score\") >= 8, \"Loyal\") \\\n",
    "    .when(col(\"rfm_score\") >= 6, \"Potential\") \\\n",
    "    .otherwise(\"At Risk\")\n",
    ")\n",
    "\n",
    "print(\"=== Segmentation RFM ===\")\n",
    "df_rfm_score.orderBy(col(\"rfm_score\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e17880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par catégorie RFM\n",
    "print(\"=== Distribution par Catégorie RFM ===\")\n",
    "df_rfm_score.groupBy(\"rfm_category\").agg(\n",
    "    count(\"customer_id\").alias(\"nb_clients\"),\n",
    "    spark_sum(\"monetary\").alias(\"ca_total\"),\n",
    "    avg(\"monetary\").alias(\"ca_moyen\")\n",
    ").orderBy(col(\"ca_total\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb049dbe",
   "metadata": {},
   "source": [
    "## 6. Customer Lifetime Value (CLV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd97422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLV = Valeur moyenne par commande × Fréquence × Durée de vie client\n",
    "df_clv = df_joined.groupBy(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\").agg(\n",
    "    count(\"order_id\").alias(\"nb_commandes\"),\n",
    "    spark_sum(\"total_amount\").alias(\"ca_total\"),\n",
    "    avg(\"total_amount\").alias(\"ca_moyen_commande\"),\n",
    "    spark_min(\"order_date\").alias(\"premiere_commande\"),\n",
    "    spark_max(\"order_date\").alias(\"derniere_commande\")\n",
    ")\n",
    "\n",
    "# Durée de vie (en jours)\n",
    "df_clv = df_clv.withColumn(\n",
    "    \"duree_vie_jours\",\n",
    "    datediff(col(\"derniere_commande\"), col(\"premiere_commande\"))\n",
    ")\n",
    "\n",
    "# Fréquence d'achat (commandes par jour)\n",
    "df_clv = df_clv.withColumn(\n",
    "    \"frequence_achat\",\n",
    "    when(\n",
    "        col(\"duree_vie_jours\") > 0,\n",
    "        spark_round(col(\"nb_commandes\") / col(\"duree_vie_jours\"), 4)\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# CLV estimé (simplifié)\n",
    "df_clv = df_clv.withColumn(\n",
    "    \"clv_estime\",\n",
    "    spark_round(col(\"ca_moyen_commande\") * col(\"nb_commandes\") * 2, 2)  # × 2 pour projection future\n",
    ")\n",
    "\n",
    "df_clv = df_clv.withColumn(\"ca_total\", spark_round(\"ca_total\", 2)) \\\n",
    "    .withColumn(\"ca_moyen_commande\", spark_round(\"ca_moyen_commande\", 2))\n",
    "\n",
    "print(\"=== Customer Lifetime Value ===\")\n",
    "df_clv.orderBy(col(\"clv_estime\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques CLV par segment\n",
    "print(\"=== CLV par Segment ===\")\n",
    "df_clv.groupBy(\"customer_segment\").agg(\n",
    "    count(\"customer_id\").alias(\"nb_clients\"),\n",
    "    avg(\"clv_estime\").alias(\"clv_moyen\"),\n",
    "    spark_max(\"clv_estime\").alias(\"clv_max\"),\n",
    "    avg(\"frequence_achat\").alias(\"freq_achat_moyen\")\n",
    ").orderBy(col(\"clv_moyen\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d681d2",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49626518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les datasets transformés\n",
    "output_path = \"../data/processed/\"\n",
    "\n",
    "# 1. Dataset jointure complète\n",
    "df_joined.write.mode(\"overwrite\").parquet(f\"{output_path}customers_orders_joined.parquet\")\n",
    "print(\" customers_orders_joined.parquet sauvegardé\")\n",
    "\n",
    "# 2. CA par client\n",
    "df_ca_client.write.mode(\"overwrite\").parquet(f\"{output_path}ca_par_client.parquet\")\n",
    "print(\" ca_par_client.parquet sauvegardé\")\n",
    "\n",
    "# 3. CA mensuel avec évolution\n",
    "df_ca_mensuel_evolution.write.mode(\"overwrite\").parquet(f\"{output_path}ca_mensuel_evolution.parquet\")\n",
    "print(\" ca_mensuel_evolution.parquet sauvegardé\")\n",
    "\n",
    "# 4. Analyse RFM\n",
    "df_rfm_score.write.mode(\"overwrite\").parquet(f\"{output_path}rfm_analysis.parquet\")\n",
    "print(\" rfm_analysis.parquet sauvegardé\")\n",
    "\n",
    "# 5. Customer Lifetime Value\n",
    "df_clv.write.mode(\"overwrite\").parquet(f\"{output_path}customer_lifetime_value.parquet\")\n",
    "print(\" customer_lifetime_value.parquet sauvegardé\")\n",
    "\n",
    "# 6. CA par catégorie\n",
    "df_ca_categorie.write.mode(\"overwrite\").parquet(f\"{output_path}ca_par_categorie.parquet\")\n",
    "print(\" ca_par_categorie.parquet sauvegardé\")\n",
    "\n",
    "# 7. CA par pays\n",
    "df_ca_pays.write.mode(\"overwrite\").parquet(f\"{output_path}ca_par_pays.parquet\")\n",
    "print(\" ca_par_pays.parquet sauvegardé\")\n",
    "\n",
    "print(\"\\n=== Tous les fichiers ont été sauvegardés avec succès ! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421aaa24",
   "metadata": {},
   "source": [
    "## 8. Résumé des Transformations Effectuées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e3e4e",
   "metadata": {},
   "source": [
    "### Jointures\n",
    "-  Jointure INNER entre customers et orders sur `customer_id`\n",
    "-  Préservation de l'intégrité référentielle\n",
    "\n",
    "### Agrégations\n",
    "-  CA par client (total, moyen, min, max)\n",
    "-  CA par pays\n",
    "-  CA par catégorie de produit\n",
    "-  CA par segment client\n",
    "-  CA mensuel avec évolution\n",
    "\n",
    "### Window Functions\n",
    "-  CA cumulé mensuel\n",
    "-  Évolution par rapport au mois précédent\n",
    "-  Ranking des clients par période\n",
    "-  Numérotation des commandes par client\n",
    "\n",
    "### Features Avancées\n",
    "-  Analyse RFM (Recency, Frequency, Monetary)\n",
    "-  Segmentation RFM (Champion, Loyal, Potential, At Risk)\n",
    "-  Customer Lifetime Value (CLV)\n",
    "-  Fréquence d'achat\n",
    "\n",
    "### Fichiers Générés\n",
    "Tous les résultats sont sauvegardés en format Parquet dans `data/processed/` pour utilisation dans la Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e40f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture de la session Spark\n",
    "# spark.stop()\n",
    "print(\"Session Spark active - Prête pour Part 3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
