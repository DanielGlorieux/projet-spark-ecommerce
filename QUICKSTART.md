# Guide de DÃ©marrage Rapide - Projet Spark E-commerce

## ğŸ¯ Objectif
Analyse du comportement d'achat des clients e-commerce avec Apache Spark

## ğŸ‘¥ Ã‰quipe
- **ILBOUDO P. Daniel Glorieux** - Data Ingestion & PrÃ©paration âœ…
- **PITROIPA Soraya** - Transformations & Jointures
- **KONE Khalis AÃ¯man** - Analyses & Visualisations

---

## âš¡ DÃ©marrage Rapide (5 minutes)

### 1. Installation

```powershell
# Se placer dans le projet
cd C:\Users\danie\Desktop\projet_spark\nouveau_projet

# CrÃ©er l'environnement virtuel
python -m venv venv

# Activer l'environnement
.\venv\Scripts\Activate.ps1

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 2. GÃ©nÃ©rer les DonnÃ©es (DÃ‰JÃ€ FAIT âœ…)

```powershell
# Les datasets sont dÃ©jÃ  gÃ©nÃ©rÃ©s dans data/raw/
# Si besoin de rÃ©gÃ©nÃ©rer:
python src\generate_datasets.py
```

**RÃ©sultat:**
- âœ… `data/raw/customers.csv` - 150,000 lignes (15.5 MB)
- âœ… `data/raw/orders.csv` - 200,000 lignes (16.1 MB)

### 3. Lancer le Notebook

```powershell
# Lancer Jupyter
jupyter notebook

# Ouvrir:
# notebooks/01_data_ingestion_cleaning.ipynb

# ExÃ©cuter toutes les cellules:
# Menu â†’ Cell â†’ Run All
```

---

## ğŸ“Š Ce que Contient le Projet

### Part 1: Data Ingestion & PrÃ©paration (Daniel) âœ…

#### âœ… Choix des Datasets
- **Customers**: 150k lignes, 11 colonnes
- **Orders**: 200k lignes, 11 colonnes
- **Jointure**: `customer_id` (clÃ© primaire/Ã©trangÃ¨re)

#### âœ… Ingestion Spark
```python
df_customers = spark.read.csv("data/raw/customers.csv", header=True, inferSchema=True)
df_orders = spark.read.csv("data/raw/orders.csv", header=True, inferSchema=True)
```

#### âœ… Nettoyage AppliquÃ©

**Customers:**
- Valeurs manquantes (phone, city) â†’ "Unknown"
- Emails dupliquÃ©s â†’ DÃ©dupliquÃ©s (garde le plus rÃ©cent)
- total_spent nÃ©gatifs â†’ SupprimÃ©s
- Dates normalisÃ©es â†’ format date Spark

**Orders:**
- quantity/price â‰¤ 0 â†’ SupprimÃ©s
- total_amount â†’ RecalculÃ© (quantity Ã— unit_price)
- order_status NULL â†’ "Pending"
- shipping_country NULL â†’ "Unknown"
- IntÃ©gritÃ© rÃ©fÃ©rentielle validÃ©e

#### âœ… RÃ©sultats
- **Customers clean**: ~147k lignes (98% conservÃ©es)
- **Orders clean**: ~194k lignes (97% conservÃ©es)
- **Formats**: CSV + Parquet (dans `data/processed/`)

---

## ğŸ“ Structure du Projet

```
nouveau_projet/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # âœ… DonnÃ©es brutes gÃ©nÃ©rÃ©es
â”‚   â”‚   â”œâ”€â”€ customers.csv             # 150k lignes
â”‚   â”‚   â””â”€â”€ orders.csv                # 200k lignes
â”‚   â””â”€â”€ processed/                    # âœ… DonnÃ©es nettoyÃ©es (output)
â”‚       â”œâ”€â”€ customers_clean.parquet
â”‚       â””â”€â”€ orders_clean.parquet
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_data_ingestion_cleaning.ipynb  # âœ… Notebook complet Part 1
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ generate_datasets.py          # âœ… GÃ©nÃ©rateur de donnÃ©es
â”‚
â”œâ”€â”€ .gitignore                        # âœ… Exclusions Git
â”œâ”€â”€ requirements.txt                  # âœ… DÃ©pendances Python
â””â”€â”€ README.md                         # âœ… Documentation complÃ¨te
```

---

## ğŸ”§ Commandes Utiles

### Gestion Environnement
```powershell
# Activer l'environnement
.\venv\Scripts\Activate.ps1

# DÃ©sactiver
deactivate

# Installer une dÃ©pendance supplÃ©mentaire
pip install <package>
```

### Jupyter
```powershell
# Lancer Jupyter
jupyter notebook

# Lancer JupyterLab (interface moderne)
jupyter lab
```

### Git
```powershell
# Voir le statut
git status

# Voir l'historique
git log --oneline

# CrÃ©er une branche pour les autres membres
git checkout -b soraya-transformations
git checkout -b khalis-analyses
```

---

## ğŸ“Š Statistiques des Datasets

### Customers (Brut â†’ Clean)
| MÃ©trique | Brut | Clean | Ã‰cart |
|----------|------|-------|-------|
| Lignes | 150,000 | ~147,000 | -2% |
| Emails uniques | 143,365 | 147,000 | +2.5% |
| Valeurs NULL (phone) | 2,958 | 0 | âœ… |
| Valeurs NULL (city) | 2,221 | 0 | âœ… |
| total_spent < 0 | 125 | 0 | âœ… |

### Orders (Brut â†’ Clean)
| MÃ©trique | Brut | Clean | Ã‰cart |
|----------|------|-------|-------|
| Lignes | 200,000 | ~194,000 | -3% |
| quantity â‰¤ 0 | 2,012 | 0 | âœ… |
| unit_price â‰¤ 0 | 2,064 | 0 | âœ… |
| order_status NULL | 939 | 0 | âœ… |
| total_amount incohÃ©rent | ~4,000 | 0 | âœ… |

---

## ğŸ¯ Prochaines Ã‰tapes (Soraya & Khalis)

### Part 2: Transformations & Jointures (Soraya)

```python
# Charger les donnÃ©es propres
df_customers = spark.read.parquet("data/processed/customers_clean.parquet")
df_orders = spark.read.parquet("data/processed/orders_clean.parquet")

# Jointure
df_joined = df_orders.join(df_customers, "customer_id", "inner")

# AgrÃ©gations
# - Chiffre d'affaires par client
# - Commandes par pays
# - Ventes par catÃ©gorie
# - Ã‰volution temporelle

# Window functions
# - RFM analysis (Recency, Frequency, Monetary)
# - Customer lifetime value
# - Ranking des produits
```

### Part 3: Analyses & Visualisations (Khalis)

```python
# Segmentation clients
# - K-means sur les features
# - Analyse par segment

# Visualisations
# - Matplotlib/Seaborn/Plotly
# - Distribution des ventes
# - Heatmaps gÃ©ographiques
# - Time series

# Recommandations business
# - Top clients Ã  fidÃ©liser
# - Produits Ã  promouvoir
# - Zones gÃ©ographiques prioritaires
```

---

## ğŸ› RÃ©solution de ProblÃ¨mes

### Erreur "Spark not found"
```powershell
pip install pyspark==3.5.0
```

### Erreur "Java not found"
- Installer Java JDK 8 ou 11
- Configurer JAVA_HOME

### Kernel Jupyter ne dÃ©marre pas
```powershell
python -m ipykernel install --user --name=venv
```

### DonnÃ©es manquantes
```powershell
# RÃ©gÃ©nÃ©rer les datasets
python src\generate_datasets.py
```

---

## ğŸ“š Ressources

### Documentation
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### Tutoriels
- [PySpark Tutorial - DataCamp](https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark)
- [Spark by Examples](https://sparkbyexamples.com/pyspark-tutorial/)

---

## âœ… Checklist Part 1 (Daniel)

- [x] Choix de 2 datasets volumineux (â‰¥100k lignes)
- [x] DÃ©finition du problÃ¨me mÃ©tier
- [x] Justification du choix
- [x] Chargement des datasets avec Spark
- [x] VÃ©rification des schÃ©mas
- [x] Gestion des valeurs manquantes
- [x] Normalisation des formats
- [x] PrÃ©paration des colonnes pour jointure
- [x] Justification des transformations
- [x] DataFrames propres et exploitables
- [x] Documentation dans README
- [x] Commit Git: `data_ingestion_cleaning`

---

## ğŸ“ Contact

**ILBOUDO P. Daniel Glorieux**  
Part 1: Data Ingestion & PrÃ©paration  
Statut: âœ… ComplÃ©tÃ©

**Membres du groupe:**
- ILBOUDO P. Daniel Glorieux
- PITROIPA Soraya
- KONE Khalis AÃ¯man

---

## ğŸ‰ FÃ©licitations!

Vous avez maintenant un pipeline de donnÃ©es propre et prÃªt pour l'analyse. Les datasets sont nettoyÃ©s, validÃ©s et optimisÃ©s pour les Ã©tapes suivantes.

**Prochaine Ã©tape**: Passer le projet Ã  Soraya et Khalis pour continuer! ğŸš€
